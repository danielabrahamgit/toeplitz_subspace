{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250e5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_memlab import MemReporter\n",
    "\n",
    "from linop import SubspaceLinopFactory\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d5acc",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6713c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ksp shape = torch.Size([600, 16, 1000]), dtype = torch.complex64\n",
      "trj shape = torch.Size([600, 2, 1000]), dtype = torch.float64\n",
      "dcf shape = torch.Size([600, 1000]), dtype = torch.float32\n",
      "phi shape = torch.Size([4, 600]), dtype = torch.complex64\n",
      "mps shape = torch.Size([16, 200, 200]), dtype = torch.complex64\n"
     ]
    }
   ],
   "source": [
    "# Load MR data\n",
    "ksp = torch.from_numpy(np.load('data/ksp.npy')).to(device) # Raw k-space data\n",
    "trj = torch.from_numpy(np.load('data/trj.npy')).to(device) # Trajectory\n",
    "dcf = torch.from_numpy(np.load('data/dcf.npy')).type(torch.float32).to(device) # Density compensation\n",
    "phi = torch.from_numpy(np.load('data/phi.npy')).type(torch.complex64).to(device) # Subspace basis\n",
    "mps = torch.from_numpy(np.load('data/mps.npy')).type(torch.complex64).to(device) # Sensitivity maps\n",
    "\n",
    "# Fix dimensions\n",
    "ksp = rearrange(ksp, 'c k 1 t -> t c k')\n",
    "trj = rearrange(trj, 'k 1 r d -> r d k') * 2 * np.pi\n",
    "dcf = rearrange(dcf, 'k 1 r -> r k')\n",
    "\n",
    "\n",
    "# Show dimensions and types\n",
    "print(f'ksp shape = {ksp.shape}, dtype = {ksp.dtype}')\n",
    "print(f'trj shape = {trj.shape}, dtype = {trj.dtype}')\n",
    "print(f'dcf shape = {dcf.shape}, dtype = {dcf.dtype}')\n",
    "print(f'phi shape = {phi.shape}, dtype = {phi.dtype}')\n",
    "print(f'mps shape = {mps.shape}, dtype = {mps.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37799ba3",
   "metadata": {},
   "source": [
    "## Create linops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9473fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Computing weights...\n",
      ">> Time: 0.0010953517630696297 s\n",
      "> Generating kernels...\n",
      ">> Calculating kernel(0, 0)\n",
      ">> Calculating kernel(1, 0)\n",
      ">> Calculating kernel(2, 0)\n",
      ">> Calculating kernel(3, 0)\n",
      ">> Calculating kernel(1, 1)\n",
      ">> Calculating kernel(2, 1)\n",
      ">> Calculating kernel(3, 1)\n",
      ">> Calculating kernel(2, 2)\n",
      ">> Calculating kernel(3, 2)\n",
      ">> Calculating kernel(3, 3)\n",
      ">> Time: 97.5351887345314 s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m A, ishape, oshape \u001b[38;5;241m=\u001b[39m linop_factory\u001b[38;5;241m.\u001b[39mget_forward()\n\u001b[1;32m      4\u001b[0m AH, _, _ \u001b[38;5;241m=\u001b[39m linop_factory\u001b[38;5;241m.\u001b[39mget_adjoint()\n\u001b[0;32m----> 5\u001b[0m AHA, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlinop_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoeplitz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/toeplitz_subspace/torch/linop.py:200\u001b[0m, in \u001b[0;36mSubspaceLinopFactory.get_normal\u001b[0;34m(self, toeplitz, oversamp_factor, device, kernels, verbose)\u001b[0m\n\u001b[1;32m    197\u001b[0m kernel_size \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39mD:]\n\u001b[1;32m    198\u001b[0m pads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(([(ksz \u001b[38;5;241m-\u001b[39m isz) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    199\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m ksz, isz \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kernel_size, im_size)), start\u001b[38;5;241m=\u001b[39m[])\u001b[38;5;241m.\u001b[39mreverse()  \u001b[38;5;66;03m# F.pad requires reversal of dims\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m slc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mslice\u001b[39m(p[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi], \u001b[38;5;241m-\u001b[39mp[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpads\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)]\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(pads)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mprint\u001b[39m(slc)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "linop_factory = SubspaceLinopFactory(trj, phi, mps, torch.sqrt(dcf))\n",
    "linop_factory.to(device)\n",
    "A, ishape, oshape = linop_factory.get_forward()\n",
    "AH, _, _ = linop_factory.get_adjoint()\n",
    "AHA, _, _ = linop_factory.get_normal(toeplitz=True, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4e6a2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element type                                            Size  Used MEM\n",
      "-------------------------------------------------------------------------------\n",
      "Storage on cuda:0\n",
      "Tensor0                                   (16, 1000, 1, 600)    73.24M\n",
      "Tensor1                                       (1000, 1, 600)     2.29M\n",
      "Tensor2                                             (4, 600)    19.00K\n",
      "Tensor3                                       (16, 200, 200)     4.88M\n",
      "Tensor4                                      (600, 16, 1000)     0.00B\n",
      "Tensor5                                       (600, 2, 1000)     9.16M\n",
      "Tensor6                                          (600, 1000)     0.00B\n",
      "trj                                           (600, 2, 1000)     0.00B\n",
      "phi                                                 (4, 600)     0.00B\n",
      "mps                                           (16, 200, 200)     0.00B\n",
      "sqrt_dcf                                         (600, 1000)     2.29M\n",
      "subsamp_idx                                           (600,)     5.00K\n",
      "Tensor7                                              (6145,)    48.50K\n",
      "Tensor8                                              (6145,)    48.50K\n",
      "Tensor9                                                 (2,)   512.00B\n",
      "Tensor10                                                (2,)   512.00B\n",
      "Tensor11                                                (2,)   512.00B\n",
      "Tensor12                                                (2,)   512.00B\n",
      "Tensor13                                             (36, 2)     1.00K\n",
      "Tensor14                                                (2,)   512.00B\n",
      "Tensor15                                                (2,)   512.00B\n",
      "Tensor16                                                (2,)   512.00B\n",
      "Tensor17                                          (200, 200)   312.50K\n",
      "Tensor18                                             (6145,)    48.50K\n",
      "Tensor19                                             (6145,)    48.50K\n",
      "Tensor20                                                (2,)   512.00B\n",
      "Tensor21                                                (2,)   512.00B\n",
      "Tensor22                                                (2,)   512.00B\n",
      "Tensor23                                                (2,)   512.00B\n",
      "Tensor24                                             (36, 2)     1.00K\n",
      "Tensor25                                                (2,)   512.00B\n",
      "Tensor26                                                (2,)   512.00B\n",
      "Tensor27                                                (2,)   512.00B\n",
      "Tensor28                                          (200, 200)   312.50K\n",
      "Tensor29                                    (4, 4, 400, 400)    19.53M\n",
      "-------------------------------------------------------------------------------\n",
      "Total Tensors: 27350152 \tUsed Memory: 112.22M\n",
      "The allocated memory on cuda:0: 121.10M\n",
      "Memory differs due to the matrix alignment or invisible gradient buffer tensors\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mark/mambaforge/envs/mrf-subspace/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:283: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/mark/mambaforge/envs/mrf-subspace/lib/python3.10/site-packages/pytorch_memlab/mem_reporter.py:95: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  fact_numel = tensor.storage().size()\n",
      "/home/mark/mambaforge/envs/mrf-subspace/lib/python3.10/site-packages/pytorch_memlab/mem_reporter.py:104: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  data_ptr = tensor.storage().data_ptr()\n"
     ]
    }
   ],
   "source": [
    "reporter = MemReporter(linop_factory)\n",
    "reporter.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb469706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
